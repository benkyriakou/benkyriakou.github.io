<?xml version="1.0" encoding="utf-8"?>

<!DOCTYPE document [
  <!ENTITY mdash "&#8212;">
]>

<document type="article">
  <title>Working with transliterations on the web</title>
  <description>A primer for displaying transliterations accurately on the web.</description>
  <slug>/posts/transliteration</slug>
  <content>
    <article>
      <title>Working with transliterations on the web</title>
      <date datetime="2015-10">October 2015</date>
      <content>
        <p>Whilst developing a new site for a client with a Yiddish collection, I learnt some interesting things about the limits of modern fonts when dealing with transliterations and rarely-used characters.</p>

        <p>The collection featured both Yiddish and transliterated strings of text in their metadata, and our task was to bring all these into the website and display them correctly and uniformly.</p>

        <h2>What is a transliteration</h2>

        <p>Before I dive in, I should explain what transliteration is, and why you’d use it. Consider this Yiddish string;</p>

        <p class="text-center"><strong>ל.נ טאלסטאי</strong></p>

        <p>If you don’t read Yiddish it’s hard to figure out how you’d begin to pronounce or search for this. Most users can read Roman characters (a-z), so converting it from Yiddish glyphs to Roman characters makes it much more legible. With a transliteration we would get this;</p>

        <p class="text-center"><strong>L.N. Ṭolsṭoy</strong></p>

        <p>Now most users could at least approximate how this would be pronounced, and likely recognise the name as that of Leo Tolstoy. Having this transliterated version in our metadata also makes it more searchable&mdash;it’s much simpler to match a search for ‘Tolstoy’ to ‘Ṭolsṭoy’ than to ‘טאלסטאי’.</p>

        <p>Transliterations are created using transliteration tables, which have mappings for various languages. These tables are published in various standard forms&mdash;one of the more common is the <a href="http://www.loc.gov/catdir/cpso/roman.html">ALA-LC standard from the Library of Congress</a>.</p>

        <h2>Back to the problem</h2>

        <p>We knew that there would be a large number of transliterated strings, so the first task was to import the metadata and see what it looked like with the chosen font. Immediately problems started cropping up&mdash;some letters looked odd, others were obviously rendered in a different font. After digging into the data, it became apparent we were dealing with two separate problems.</p>

        <h2>Character composition</h2>

        <p>The first problem&mdash;the odd-looking letters&mdash;was caused by the presence of both pre- and de-composed characters in the metadata. Character composition is a feature of Unicode which can be used when rendering characters with diacritics such as ‘ṭ’. There are two ways to represent this character;</p>

        <p class="text-center">
          <code>LATIN SMALL LETTER T WITH DOT BELOW (U+1E6D)</code>
          <br/>
          <code>LATIN SMALL LETTER T (U+0074) + COMBINING DOT BELOW (U+0323)</code>
        </p>

        <p>The first representation is known as a pre-composed character (all features are included in a single glyph), and the second as a de-composed character (the letter and diacritic are two separate glyphs). For de-composed characters, the way the character is rendered varies wildly between fonts, and is very dependent on the font design.</p>

        <h2>Testing</h2>

        <p>To identify what the best solution would be, I put together a test-case page containing all of our known diacritic variants, and tested it with a selection of fonts on both Windows and OSX. It was immediately apparent that the pre-composed variant displayed most consistently, so we made the decision to normalise all the strings. As it turned out this was a straightforward transformation&mdash;we were already processing the data with Python, and the <code>unicodedata</code> library lets you do this:</p>

        <p class="text-center"><code>unicodedata.normalize('NFC', 'decomposed_string')</code></p>

        <p>Apart from some edge cases like combining ligatures (a topic for another time) this worked very well.</p>

        <h2>Finding a font</h2>

        <p>The second issue, of characters not rendering in our desired font face, was due to missing characters in the font. This is something to be aware of when venturing into the domain of rarely-used characters&mdash;font designers will rarely include every possible character in their font, but will instead design glyphs for only the most commonly used characters.</p>

        <p>Our client got in touch with the font designer and arranged to have the missing characters added. However, we still had an issue whilst this took place. As an interim solution we looked around for a compatible font&mdash;<a href="https://www.google.com/get/noto/">Google Noto</a> was our final choice, as it supports a huge range of characters, and this worked very well.</p>

        <h2>Lessons learned</h2>

        <p>This was a very interesting process, and I learnt a lot from it: don’t assume that your font will support all of the glyphs that you require if you’re working outside of common ASCII characters; test a known set of non-standard characters on all supported platforms; and if you don’t require de-composed characters for other purposes, pre-composition will help your data display more consistently.</p>

        <p>This is by no means an exhaustive guide to potential issues with fonts and characters&mdash;if this talk of Unicode and ASCII sounds esoteric I’d recommend <a href="http://www.joelonsoftware.com/articles/Unicode.html">Joel Spolsky’s guide to character encoding and Unicode</a> (a must-read for all programmers). If you’ve had similar issues, I’d love to know how you’ve solved them&mdash;let me know on Twitter via <a href="https://twitter.com/benkyriakou">@benkyriakou</a>.</p>
      </content>
    </article>
  </content>
</document>
